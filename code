# ================================
# TITANIC SURVIVAL PREDICTION
# Full End-to-End ML Notebook Code
# ================================

# -------- 1. Import Libraries --------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

%matplotlib inline


# -------- 2. Load Dataset --------
df = pd.read_csv("C:\\Users\\DELL\\OneDrive\\Desktop (1)\\INTERNSHIP\\titanic.csv")

print("First 5 rows:")
display(df.head())


# -------- 3. Dataset Overview --------
print("\nDataset Info:")
df.info()

print("\nStatistical Summary:")
display(df.describe())

print("\nMissing Values:")
print(df.isnull().sum())


# -------- 4. Exploratory Data Analysis (EDA) --------

# Survival Count
plt.figure()
sns.countplot(x='Survived', data=df)
plt.title("Survival Count")
plt.show()

# Survival by Gender (ERROR-FREE)
df_plot = df.copy()
df_plot['Survived'] = df_plot['Survived'].map({0: 'No', 1: 'Yes'})

plt.figure()
sns.countplot(x='Sex', hue='Survived', data=df_plot)
plt.title("Survival by Gender")
plt.show()

# Age Distribution
plt.figure()
sns.histplot(df['Age'], bins=30, kde=True)
plt.title("Age Distribution")
plt.show()


# -------- 5. Data Cleaning --------
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)


# -------- 6. Encoding Categorical Variables --------
df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)

print("\nData after encoding:")
display(df.head())


# -------- 7. Feature Selection --------
X = df.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1)
y = df['Survived']


# -------- 8. Train-Test Split --------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# -------- 9. Logistic Regression Model --------
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

lr_pred = lr.predict(X_test)

print("\nLogistic Regression Results")
print("Accuracy:", accuracy_score(y_test, lr_pred))
print(classification_report(y_test, lr_pred))


# -------- 10. Random Forest Model --------
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

rf_pred = rf.predict(X_test)

print("\nRandom Forest Results")
print("Accuracy:", accuracy_score(y_test, rf_pred))
print(classification_report(y_test, rf_pred))


# -------- 11. Hyperparameter Tuning (GridSearchCV) --------
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, None]
}

grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5
)

grid.fit(X_train, y_train)


# -------- 12. Best Model Evaluation --------
best_model = grid.best_estimator_
best_pred = best_model.predict(X_test)

print("\nTuned Random Forest Results")
print("Improved Accuracy:", accuracy_score(y_test, best_pred))
print(classification_report(y_test, best_pred))


# -------- 13. Conclusion --------
print("Model training and evaluation completed successfully!")
